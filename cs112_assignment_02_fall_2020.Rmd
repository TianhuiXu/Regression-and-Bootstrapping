---
title: "CS112 Assignment 2, Fall 2020"
author: "Enter your name here"
date: "09/27/2020"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
# Don't change this part of the document
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
## install and load the necessary packages
library(lubridate)
library(tree)
library(Matching)
library(boot)
library(randomForest)
library(ggplot2)
library(arm)
library(rgenoud)
# we need to set the seed of R's random number generator, in order to produce comparable results 
set.seed(32)
```

**Note**: *This is an RMarkdown document. Did you know you can open this document in RStudio, edit it by adding your answers and code, and then knit it to a pdf? Then you can submit both the .rmd file (the edited file) and the pdf file as a zip file on Forum. This method is actually preferred. To learn more about RMarkdown, watch the videos from session 1 and session 2 of the CS112B optional class. [This](https://rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf) is also a cheat sheet for using Rmarkdown. If you have questions about RMarkdown, please post them on Perusall.*

**Note**: *If you are not comfortable with RMarkdown, you can use any text editor (google doc or word) and type your answers and then save everything as a pdf and submit both the pdf file AND the link to your code (on github or jupyter) on Forum.*

**Note**: *Try knitting this document in your RStudio. You should be able to get a pdf file. At any step, you can try knitting the document and recreate a pdf. If you get error, you might have incomplete code.*

**Note**: *If you are submitting your assignment as an RMarkdown file, make sure you add your name to the top of this document.*

## QUESTION 1

#### STEP 1

Create a set of 1000 outcome observations using a data-generating process (DGP) that incorporates a systematic component and a stochastic component (of your choice)

```{r}
set.seed(11)
# independent variable1: geographical distance between cities currently located in
geo_dis <- runif(1000,0,20000)

# independent variable2: cultural proximity index (100 when coming from the same culture)
cul_prox <- runif(1000) * 100

# independent variable3: friends have in common
friend_num <- as.integer(runif(1000) * 200)

irreducible_error <- rnorm(1000,5,5)

friendship_idx <- -(1/100)*(geo_dis)  + 80 * cul_prox + 12 * (friend_num) + irreducible_error

df <- data.frame(geo_dis, cul_prox, friend_num, friendship_idx)
head(df)
```

#### STEP 2

Tell a 2-3 sentence story about the data generating process you coded up above. What is it about and what each component mean?

This dataset illustrates an imaginary relationship of the level of friendship between two Minervans after graduation and their geographical distance, number of mutual friends, cultural proximity.

The dependent variable is the level of friendship between two Minervans a years after graduation and the other variables are the independent variables: 
- geographical distance (km): How far away is their base from each other.
- cultural proximity (index, 1-100): A measurement of the similarity between the culture that they're from.
- number of friends in common (#): How many friends they have in common.

And the random distribution function part accounts for the random noise.


#### STEP 3

Using an incorrect model of the systematic component (of your choice), and using the simulation-based approach covered in class (the `arm` library, etc.), generate 1 prediction for every observation and calculate and report RMSE. Make sure you write out your model and report your RMSE. 

Each prediction should be conditional on the predictor values of the corresponding observation. E.g., when predicting for observation #1, use the predictor values of observation #1.
```{r}
incorrect = df
# generate an incorrect model of the systematic component
lm <- lm(friendship_idx ~ geo_dis + cul_prox + friend_num, data = df)
sim <- sim(lm,1)
coef <- sim@coef
# report the model
sprintf("The simulated model of the systematic component is: Friendship_index = %f + %f * geographical_distance + %f * cultural_proximity + %f * friend_number", coef[1], coef[2], coef[3], coef[4])
```
```{r}
# generate the one prediction based on the coefficient simulated
for (i in 1:1000) {
  incorrect[i,4] <- coef[1] + coef[2] * incorrect[i,1] + coef[3] * incorrect[i,2] + coef[4] * incorrect[i,3]
}
names(incorrect)[4] = "simulated outcome"

# calculate RMSE
RMSE_incorrect <- sqrt(mean((df[,4] - incorrect[,4])**2))
sprintf("The RMSE for the incorrect modelis: %f.", RMSE_incorrect)
```

#### STEP 4
Using the correct model (correct systematic and stochastic components), and using the simulation-based approach covered in class (the `arm` library, etc.), generate 1 prediction for every observation and calculate & report your RMSE. Once again, write out your model and report your RMSE. 

Each prediction should be conditional on the predictor values of the corresponding observation. E.g., when predicting for observation #1, use the predictor values of observation #1.

```{r}
correct = df
correct[,4] = 0
# generate an correct model of the systematic component and stochastic components
sigma <- sim@sigma
# report the model
sprintf("The simulated model of the systematic component and stochastic components is: Friendship_index = %f + %f * geographical_distance + %f * cultural_proximity + %f * friend_number + %f", coef[1], coef[2], coef[3], coef[4], sigma)
```

```{r}
# generate the one prediction based on the coefficient and error simulated
for (i in 1:1000) {
  correct[i,4] <- coef[1] + coef[2] * correct[i,1] + coef[3] * correct[i,2] + coef[4] * correct[i,3] + sigma
}
names(correct)[4] = "simulated outcome"

# calculate RMSE
RMSE_correct <- sqrt(mean((df[,4] - correct[,4])**2))
sprintf("The RMSE for the correct model is: %f.", RMSE_correct)
```


#### STEP 5

Which RMSE is larger: The one from the correct model or the one from the incorrect model? Why?

In this experiment, the correct model contains both the systematic and stochastic part, whereas the incorrect model only contains the systematic part. And the RMSE from the correct model is larger than the one from the incorrect model, which is not correct intuitively.

Theoretically, the RMSE from the incorrect model should be larger, because RMSE, the root mean squared error, represents the difference between the predicted value and the actual value, so the correct model, also taking into consideration the stochastic part, would have more precise predicted values. 

But in this case, the sigma we use as a stochastic part is actually the residual standard deviation, which is not really a good estimation of the error term, causing the RMSE to become larger. 


## QUESTION 2

Imagine that you want to create a data viz that illustrates the sensitivity of regression to outlier data points. So, you want to create two figures: 
	
One figure that shows a regression line fit to a 2-dimensional (x and y) scatterplot, such that the regression line clearly has a positive slope. 

```{r}
set.seed(22)
data = data.frame()
for (i in seq(1,50)) {
  data[i,1] = i
  data[i,2] = i * 20 + 50 + rnorm(1) * 200
}
names(data)[1] = "independent_variable"
names(data)[2] = "dependent_variable"
plot(data, main = "Regression line with a positive slop", xlab = "Independent variable", ylab = "Dependent variable")
lm2 <- lm(dependent_variable ~ independent_variable, data = data)
abline(lm2,lwd=3,col="red")
```

And, another figure that shows a regression line with a negative slope fit to a scatter plot of the same data **plus one additional outlier data point**. This one data point is what changes the sign of the regression line’s slope from positive to negative.

```{r}
data = data.frame()
for (i in seq(1,50)) {
  data[i,1] = i
  data[i,2] = i * 20 + 50 + rnorm(1) * 200
}
names(data)[1] = "independent_variable"
names(data)[2] = "dependent_variable"
data[1,2] = 25000
plot(data, main = "Change of regression line slope when adding an outlier", xlab = "Independent variable", ylab = "Dependent variable")
lm3 <- lm(dependent_variable ~ independent_variable, data = data)
abline(lm3,lwd=3,col="red")
```

Be sure to label the axes and the title the figures appropriately. Include a brief paragraph that explains the number of observations and how you created the data set and the outlier.

There are 50 observations in total. The data set is generated by first defining a linear relationship between the dependent and independent variable and adding an error term using random normal distribution. 

The outlier has an x value of 1 and an extreme y value of 25000 which doesn't follow the general trend of the rest of the data. It's created to be influential that it also takes on the smallest x value to increase the leverage, so that the regression line fit with the outlier and without the outlier is very different and changes slope.




## QUESTION 3

#### STEP 1

Using the `laLonde` data set, run a linear regression that models `re78` as a function of `age`, `education`, `re74`, `re75`, `hisp`, and `black`. Note that the `lalonde` data set comes with the package `Matching`.

```{r}
data(lalonde)
# fit a linear regression model
lm1 <- lm(re78~age+educ+re74+re75+hisp+black, data=lalonde)
summary(lm1)
```

#### STEP 2

Report coefficients and R-squared. 

Then calculate R-squared by hand and confirm / report that you get the same or nearly the same answer as the summary (`lm`) command. 

Write out hand calculations here.
```{r}
# report coefficients and R-sqaured
print("The coefficients estimated from the model are:") 
print(coef(lm1))
print("The reported R-squared is: 0.039")
```
```{r}
# predict using the fitted model
predicted_re78 <- predict.lm(lm1, lalonde)

# hand-calculation of R-squared
mean <- mean(lalonde$re78)
# SSR (regression sum of squares): how far the estimated sloped regression line is from the horizontal 'no relationship line', the sample mean
SSR <- sum((predicted_re78 - mean) ** 2)
# SSTO (total sum of squres): how much the data points, vary around their mean
SSTO <- sum((lalonde$re78 - mean) ** 2)
SSE <- sum((lalonde$re78 - predicted_re78) ** 2)
R_squared <- SSR/SSTO
sprintf("The calculated R-squared by hand is: %s, the answer is the same with the R-squared from the summary statistics above.",round(R_squared,3))
```

#### STEP 3

Then, setting all the predictors at their means EXCEPT `education`, create a data visualization that shows the 95% confidence interval of the expected values of `re78` as `education` varies from 3 to 16. Be sure to include axes labels and figure titles.

```{r}
set.seed(254)
# setting all predictors at their mean
age = mean(lalonde$age)
re74 = mean(lalonde$re74)
re75 = mean(lalonde$re75)
hisp = mean(lalonde$hisp)
black = mean(lalonde$black)

# create a function to calculate the dependent variable given coefficients and independent variables
calc_turnout <- function(coef, person) {
  turnout <- coef[1] + 
    coef[2] * person[1] + 
    coef[3] * person[2] + 
    coef[4] * person[3] + 
    coef[5] * person[4] + 
    coef[6] * person[5] + 
    coef[7] * person[6]
  return(turnout)
}
```
```{r}
# use three nested loops to get the expected values of 're78'
storage.matrix_exp <- matrix(NA, nrow = 1000, ncol = 14)

for (j in c(1:1000)) {
  sim_exp <- sim(lm1, 1000)
  sim_coef_exp <- sim_exp@coef 
  for (educ in c(3:16)) {
    person <- c(age, educ, re74, re75, hisp, black)
    store <- rep(0,1000)
    for (i in c(1:1000)) {
      store[i] <- calc_turnout(sim_coef_exp[i,],person) 
    }
    storage.matrix_exp[j,educ-2] <- mean(store)
  }
}

# 95% confidence interval of the expected values
conf.intervals_exp <- apply(storage.matrix_exp, 2, quantile, probs = c(0.025, 0.975))

plot(x = c(1:100), y = c(1:100), type = "n", xlim = c(3,16), ylim = c(1,10000), 
     main = "Expected Earnings in 1978 by Years of Schooling (95% CI)", xlab = "Years of Schooling", 
     ylab = "Real Earnings in 1978")

for (educ in 3:16) {
  segments(
    x0 = educ,
    y0 = storage.matrix_exp[1, educ - 2],
    x1 = educ,
    y1 = storage.matrix_exp[2, educ - 2],
    lwd = 2)
}
```

#### STEP 4

Then, do the same thing, but this time for the predicted values of `re78`. Be sure to include axes labels and figure titles.

```{r}
set.seed(25)
# simulate 1000 set of coefficients 
sim_pred <- sim(lm1, 1000)
sim_coef_pred <- sim_pred@coef

# store 1000 predicted values for each education value
storage.matrix_pred <- matrix(NA, nrow = 1000, ncol = 14)
for (educ in c(3:16)) {
  for (i in c(1:1000)) {
    person <- c(age, educ, re74, re75, hisp, black)
    storage.matrix_pred[i,educ-2] <- calc_turnout(sim_coef_pred[i,],person)
  }
}
conf.intervals_pred <- apply(storage.matrix_pred, 2, quantile, probs = c(0.025, 0.975))

plot(x = c(1:100), y = c(1:100), type = "n", xlim = c(3,16), ylim = c(1,10000), 
     main = "Predicted Earnings in 1978 by Years of Schooling (95% CI)", xlab = "Years of Schooling", 
     ylab = "Real Earnings in 1978")

for (educ in 3:16) {
  segments(
    x0 = educ,
    y0 = storage.matrix_pred[1, educ - 2],
    x1 = educ,
    y1 = storage.matrix_pred[2, educ - 2],
    lwd = 2)
}


```
#### STEP 5

Lastly, write a short paragraph with your reflections on this exercise (specifically, the length of intervals for given expected vs. predicted values) and the results you obtained.

The confidence intervals of the expected values are smaller than the predicted values. For the expected values, the spread of the values is narrower because taking the mean of the 1000 predictions reduces the variance so that the values are centered towards the mean.

The results is that years of schooling (3-16) has a positive influence when predicting real earnings in 1978 given that the other predictors are at their mean, and the relationship is linear. The confidence intervals of the expected predicted values decreases when years of schooling approaches 9 and increases after that, meaning that it's most certain when years of schooling is around 9 and uncertain around the two extremes. The expected earning of 3 years of schooling is around \$2000, and increase linearly towards around \$8000 for 16 years of schooling.


## QUESTION 4

#### STEP 1

Using the `lalonde` data set, run a logistic regression, modeling treatment status as a function of `age`, `education`, `hisp`, `re74` and `re75`. Report and interpret the regression coefficient and 95% confidence intervals for `age` and `education`.

```{r message=FALSE, warning=FALSE}
glm.treat <- glm(treat~age+educ+hisp+re74+re75,data=lalonde,family=binomial)
print("The regression coefficients are:")
print(coef(glm.treat))
conf_int <- confint(glm.treat)
sprintf("The 95%% confidence interval for 'age' is [%f,%f].", conf_int[2,1], conf_int[2,2])
sprintf("The 95%% confidence interval for 'education' is [%f,%f].", conf_int[3,1], conf_int[3,2])
```

Report and interpret regression coefficient and 95% confidence intervals for `age` and `education` here. 
Logistic regression predicts the probability of the dependent variable belongs to a certain category given the independent variables. The coefficients of each independent variable in the model is derived from 

The regression coefficient shows that a unit increase in *age* results in an *increase* in the log odds of treatment by 2.816\*(10^-3), a unit increase in *years of schooling* results in an *increase* in the log odds of treatment by 1.629\*(10^-2). However, a unit increase in *hispanic* (0-1) results in a *decrease* in the log odds of treatment by 1.336 * 10^(-1), and a unit increase in *earnings in 1974* results in a *decrease* in the log odds of treatments by 5.235 * 10^(-6). A unit increase in *earnings in 1975* results in a *increase* in the log odds of treatment by 1.226 * 10^(-5).

The confidence intervals for 'age' and 'education' means that 95% of the estimated coefficients of the two variables fall in the two intervals.



#### STEP 2

Use a simple bootstrap to estimate (and report) bootstrapped confidence intervals for `age` and `education` given the logistic regression above. Code the bootstrap algorithm yourself.

```{r}
set.seed(42)
age.fn <- function(data,index) {
  d <- data[index,]
  return(coef(glm(treat~age+educ+hisp+re74+re75,data=d,family=binomial))[2])
}

boot.age <- boot(lalonde, age.fn, 1000)
age_conf <- quantile(boot.age$t, probs = c(0.025, 0.975))
```
```{r}
educ.fn <- function(data,index) {
  d <- data[index,]
  return(coef(glm(treat~age+educ+hisp+re74+re75,data=d,family=binomial))[3])
}

boot.educ <- boot(lalonde, educ.fn, 1000)
educ_conf <- quantile(boot.educ$t, probs = c(0.025, 0.975))
```

Report bootstrapped confidence intervals for `age` and `education` here. 
```{r}
sprintf("The bootstrapped confidence interval for 'age' is [%f, %f].", age_conf[1], age_conf[2])
sprintf("The bootstrapped confidence interval for 'education' is [%f, %f].", educ_conf[1], educ_conf[2])
```



#### STEP 3

Then, using the simulation-based approach and the `arm` library, set all the predictors at their means EXCEPT `education`, create a data visualization that shows the 95% confidence interval of the expected values of the probability of receiving treatment as education varies from 3 to 16. Be sure to include axes labels and figure titles.

```{r}
set.seed(43)
glm.treat <- glm(treat~age+educ+hisp+re74+re75,data=lalonde,family=binomial)

# create a function to calculate the dependent variable given coefficients and independent variables
calc_turnout_2 <- function(coef, person) {
  turnout <- coef[1] + 
    coef[2] * person[1] + 
    coef[3] * person[2] + 
    coef[4] * person[3] + 
    coef[5] * person[4] + 
    coef[6] * person[5]
  return(exp(turnout) / (1 + exp(turnout)))
}

# store 1000 predicted values for each education value
storage.matrix_exp_2 <- matrix(NA, nrow = 1000, ncol = 14)
for (j in c(1:1000)) {
  sim_exp <- sim(glm.treat, 1000)
  sim_coef_exp <- sim_exp@coef 
  for (educ in c(3:16)) {
    person <- c(age, educ, hisp, re74, re75)
    store <- rep(0,1000)
    for (i in c(1:1000)) {
      store[i] <- calc_turnout_2(sim_coef_exp[i,],person) 
    }
    storage.matrix_exp_2[j,educ-2] <- mean(store)
  }
}

conf.intervals_exp_2 <- apply(storage.matrix_exp_2, 2, quantile, probs = c(0.025, 0.975))

plot(x = c(1:100), y = c(1:100), type = "n", xlim = c(3,16), ylim = c(0,1), 
     main = "Expected probability of receiving treatment by years of schooling(95%CI)", xlab = "Years of Schooling", 
     ylab = "Probability of Receiving Treatment")

for (educ in 3:16) {
  segments(
    x0 = educ,
    y0 = storage.matrix_exp_2[1, educ - 2],
    x1 = educ,
    y1 = storage.matrix_exp_2[2, educ - 2],
    lwd = 2)
}

```

#### STEP 4

Then, do the same thing, but this time for the predicted values of the probability of receiving treatment as education varies from 3 to 16. Be sure to include axes labels and figure titles.

```{r}
set.seed(44)
# simulate 1000 set of coefficients 
sim_pred_2 <- sim(glm.treat, 1000)
sim_coef_pred_2 <- sim_pred_2@coef

# create a function to calculate the dependent variable given coefficients and independent variables
calc_turnout_2 <- function(coef, person) {
  turnout <- coef[1] + 
    coef[2] * person[1] + 
    coef[3] * person[2] + 
    coef[4] * person[3] + 
    coef[5] * person[4] + 
    coef[6] * person[5]
  return(exp(turnout) / (1 + exp(turnout)))
}

# store 1000 predicted values for each education value
storage.matrix_pred_2 <- matrix(NA, nrow = 1000, ncol = 14)
for (educ in c(3:16)) {
  for (i in c(1:1000)) {
    person <- c(age, educ, hisp, re74, re75)
    storage.matrix_pred_2[i,educ-2] <- calc_turnout_2(sim_coef_pred_2[i,],person)
  }
}

conf.intervals_pred_2 <- apply(storage.matrix_pred_2, 2, quantile, probs = c(0.025, 0.975))

plot(x = c(1:100), y = c(1:100), type = "n", xlim = c(3,16), ylim = c(0,1), 
     main = "Predicted probability of receiving treatment by years of schooling(95%CI)", xlab = "Years of Schooling", 
     ylab = "Probability of Receiving Treatment")

for (educ in 3:16) {
  segments(
    x0 = educ,
    y0 = storage.matrix_pred_2[1, educ - 2],
    x1 = educ,
    y1 = storage.matrix_pred_2[2, educ - 2],
    lwd = 2)
}

```

#### STEP 5

Lastly, write a short paragraph with your reflections on this exercise and the results you obtained.

The graph is similar with the previous exercise, with confidence intervals of the expected value smaller than the predicted values. The expected probability of receiving treatment for people with 3 years of schooling is around 0.3, and increase linearly as the years of schooling increases, towards around 0.5 for 16 years of schooling. The interval for all possible independent values is similar meaning that the uncertainty is similar.


## QUESTION 5


Write the executive summary for a decision brief about the impact of a stress therapy program, targeted at individuals age 18-42, intended to reduce average monthly stress. The program was tested via RCT, and the results are summarized by the figure that you get if you run this code chunk:

```{r}
# Note that if you knit this document, this part of the code won't 
# show up in the final pdf which is OK. We don't need to see the code
# we wrote.

# How effective is a therapy method against stress

# Participants in the study record their stress level for a month.
# Every day, participants assign a value from 1 to 10 for their stress level. 
# At the end of the month, we average the results for each participant.

#adds the confidence interval (first row of the matrix is lower 
# bound, second row is the upper bound)
trt1 = matrix(NA,nrow=2,ncol=7)
ctrl = matrix(NA,nrow=2,ncol=7) 

trt1[,1]=c(3.7, 6.5) #18  
ctrl[,1]=c(5, 8)

trt1[,2]=c(5, 8.5) #22
ctrl[,2]=c(7.5, 9)

trt1[,3]=c(6, 9) #26
ctrl[,3]=c(8.5, 10)

trt1[,4]=c(5, 7) #30
ctrl[,4]=c(6, 8)

trt1[,5]=c(3.5, 5) #34
ctrl[,5]=c(4.5, 7)

trt1[,6]=c(2, 3.5) #38
ctrl[,6]=c(3.5, 6)

trt1[,7]=c(0.5, 2) #42
ctrl[,7]=c(2.5, 5)

# colors to each group
c1 = rgb(red = 0.3, green = 0, blue = 1, alpha = 0.7) #trt1
c2 = rgb(red = 1, green = 0.6, blue = 0, alpha = 1) #trt2
c3 = rgb(red = 0, green = 0.5, blue = 0, alpha = 0.7) #ctrl

# creates the background of the graph
plot(x = c(1:100), y = c(1:100), 
     type = "n", 
     xlim = c(17,43), 
     ylim = c(0,11), 
     cex.lab=1,
     main = "Stress Level - 95% Prediction Intervals", 
     xlab = "Age", 
     ylab = "Average Stress Level per Month", 
     xaxt = "n")

axis(1, at=seq(18,42,by=4), seq(18, 42, by=4))

grid(nx = NA, ny = NULL, col = "lightgray", lty = "dotted",
     lwd=par("lwd"), equilogs = TRUE)

# adds the legend
legend('topright',legend=c('Treatment','Control'),fill=c(c1,c2))

# iterates to add stuff to plot
for (age in seq(from=18,to=42,by=4)) { 
  #treatment
  segments(x0=age-0.2, y0=trt1[1, (age-18)/4+1],
           x1=age-0.2, y1=trt1[2, (age-18)/4+1], lwd=4, col=c1)
  
  #control
  segments(x0=age+0.2, y0=ctrl[1, (age-18)/4+1],
           x1=age+0.2, y1=ctrl[2, (age-18)/4+1], lwd=4, col=c2)
}

```

(Not that it matters, really, but you can imagine that these results were obtained via simulation, just like the results you have hopefully obtained for question 2 above). 

Your executive summary should be between about 4 and 10 sentences long, it should briefly describe the purpose of the study, the methodology, and the policy implications/prescription. (Feel free to imaginatively but realistically embellish/fill-in-the-blanks with respect to any of the above, since I am not giving you backstory here).

Write your executive summary here.

The purpose of the study is to test the impact of the therapy program on whether it has a significant impact on reducing the stress level, targeted at individuals age 18-42.

The study is done via an RCT trial, where a group of targeted individuals is randomly selected and assigned to a treatment group receiving the therapy program and control group receiving a placebo program. The whole experiment is done under the protocol. The participants assign a value from 1 to 10 for their stress level (with standardized metrics provided), and the result is an average of their stress level over the one-month duration.

The result of the experiment is that the therapy program on average decreases people's stress level by 2. There is a larger effect detected for people aged 42 with a gap of 3 and a smaller effect for people aged 30 with a gap of 1. But note that people aged 18 to 26 have a wider spread of stress levels with a level of 1.5 plus or minus the average.

The results implicate that the program is useful in reducing stress if the desired effect is to reduce the stress level by 2.


## QUESTION 6

Can we predict what projects end up being successful on Kickstarter? 

We have data from the [Kickstarter](https://www.kickstarter.com/) company. 

From Wikipedia: Kickstarter is an American public-benefit corporation based in Brooklyn, New York, that maintains a global crowdfunding platform focused on creativity and merchandising. The company's stated mission is to "help bring creative projects to life". As of May 2019, Kickstarter has received more than $4 billion in pledges from 16.3 million backers to fund 445,000 projects, such as films, music, stage shows, comics, journalism, video games, technology, publishing, and food-related projects.

The data is collected by [Mickaël Mouillé](https://www.kaggle.com/kemical) and is last uodated in 2018. Columns are self explanatory. Note that `usd_pledged` is the column `pledged` in US dollars (conversion done by kickstarter) and `usd_pledge_real` is the `pledged` column in real US dollars of the pledged column. Finally, `usd_goal_real` is the column `goal` in real US dollars. You should use the real columns.


So what makes a project successful? Undoubtedly, there are many factors, but perhaps we could set up a prediction problem here, similar to the one from the bonus part of the last assignment where we used GDP to predict personnel contributions. 

We have columns representing the number of backers, project length, the main category, and the real project goal in USD for each project. 

Let's explore the relationship between those predictors and the dependent variable of interest — the success of a project. 

Instead of running a simple linear regression and calling it a day, let's use cross-validation to make our prediction a little more sophisticated. 

Our general plan is the following: 

1. Build the model on a training data set 
2. Apply the model on a new test data set to make predictions based on the inferred model parameters. 
3. Compute and track the prediction errors to check performance using the mean squared difference between the observed and the predicted outcome values in the test set. 

Let's get to it, step, by step. Make sure you have loaded the necessary packages for this project. 

#### STEP 1: Import & Clean the Data

Import the dataset from this link: https://tinyurl.com/KaggleDataCS112 

Remove any rows that include missing values. 

```{r}
# import the dataset
ks_df <- read.csv("ks-projects-201801.csv")

ks_df[ks_df==""] <- NA   # set all the blanks to NAs
ks_df <- na.omit(ks_df)    # omit the NA values

# see the head
head(ks_df)
```

#### STEP 2: Codify outcome variable

Create a new variable that is either successful or NOT successful and call it `success` and save it in your dataframe. It should take values of 1 (successful) or 0 (unsuccessful).

```{r}
# A project is successful if the real pledged value is larger than or equal to the goal value
# create a column of dummy variables to indicate the status
ks_df$success <- ifelse(ks_df$usd_pledged_real - ks_df$usd_goal_real >= 0, 1, 0)
head(ks_df)
```

#### STEP 3: Getting the project length variable  

Projects on Kickstarter can last anywhere from 1 - 60 days. Kickstarter claims that projects lasting any longer are rarely successful and campaigns with shorter durations have higher success rates, and create a helpful sense of urgency around your project. Using the package `lubridate` or any other package in R you come across by Googling, create a new column that shows the length of the project by taking the difference between the variable `deadline` and the variable `launched`. Call the new column `length` and save it in your dataframe.

Remove any project length that is higher than 60. 

```{r}
# extract the date using lubricate package
ks_df$deadline = date(ks_df$deadline)
ks_df$launched = date(ks_df$launched)
# create a new column to store the length of the project
ks_df$length <- ks_df$deadline - ks_df$launched
# remove any project length that is higher than 60
ks_df <- ks_df[!(ks_df$length > 60),]
```

#### STEP 4: Splitting the data into a training and a testing set

While there are several variations of the k-fold cross-validation method, let’s stick with the simplest one where we just split randomly the dataset into two (k = 2) and split our available data from the link above into a training and a testing (aka validation) set. 

Randomly select 80% of the data to be put in a training set and leave the rest for a test set. 

```{r}
# randomly select 80% of the data 
tr_idx <- sample(nrow(ks_df),as.integer(nrow(ks_df) * 0.80))
ks_tr <- ks_df[tr_idx,]  # training set
ks_te <- ks_df[-tr_idx,]  # rest to testing set
```

#### STEP 5: Fitting a model 

Use a logistic regression to find what factors determine the chances a project is successful. Use the variable indicating whether a project is successful or not as the dependent variables (Y) and number of backers, project length, main category of the project, and the real project goal as independent variables. Make sure to use the main category as factor.

```{r message=FALSE, warning=FALSE}
ks_df$main_category <- as.factor(ks_df$main_category)

# fit a logistic regression model to the dataset
glm_ks <- glm(success ~ backers + length + main_category + usd_goal_real, data = ks_tr, family=binomial)
summary(glm_ks)
```


#### STEP 6: Predictions

Use the model you’ve inferred from the previous step to predict the success outcomes in the test set.

```{r}
# predict the response of probability of the test set using the fitted model
ks_p <- predict(glm_ks, ks_te, type = "response")
# convert into class labels
ks.pred=rep("No",nrow(ks_te))
ks.pred[ks_p > .5] = "Yes"
ks_te$ks.pred = ks.pred
# produce a confusion matrix
acc_table <- table(ks.pred,ks_te$success)
acc_table
```

#### STEP 7: How well did it do? 

Report the misclassification rate of the predictions for the training and the test sets. 

```{r}
# use the same process to predict for the training set
ks_p_tr <- predict(glm_ks, ks_tr, type = "response")
ks.pred_tr <- rep("No",nrow(ks_tr))
ks.pred_tr[ks_p_tr>.5] = "Yes"
ks_tr$ks.pred_tr = ks.pred_tr
acc_table_tr <- table(ks.pred_tr,ks_tr$success)
acc_table_tr
```
```{r}
# report misclassification rates
misclass_tr <- (acc_table_tr[1,2]+acc_table_tr[2,1])/sum(acc_table_tr)
sprintf("The misclassification rate of the predictions for the training sets is: %f",misclass_tr)
misclass_te <- (acc_table[1,2]+acc_table[2,1])/sum(acc_table)
sprintf("The misclassification rate of the predictions for the test sets is: %f",misclass_te)
```

#### Step 8: LOOCV method

Apply the leave-one-out cross validation (LOOCV) method to the training set. What is the misclassification rate of the training and test sets. How similar are the misclassification rates?

```{r message=FALSE, warning=FALSE}
# sample 5% of the data set to apply LOOCV in order to reduce running time
ks_df_sam <- ks_df[sample(nrow(ks_df),as.integer(nrow(ks_df)*0.05)),]

# randomly select 80% of the data 
sam_tr_idx <- sample(nrow(ks_df_sam),as.integer(nrow(ks_df_sam) * 0.80))
ks_sam_tr <- ks_df_sam[sam_tr_idx,]  # training set from 5% sample
ks_sam_te <- ks_df_sam[-sam_tr_idx,]  # rest to testing set from 5% sample

# fit a logistic model on the training data set
glm_ks_sam <- glm(success ~ backers + length + main_category + usd_goal_real, data = ks_sam_tr, family=binomial)

# apply LOOCV method to the training set
cv.err=cv.glm(ks_sam_tr,glm_ks_sam)
```
```{r message=FALSE, warning=FALSE}
# apply LOOCV method to the test set
cv.err.test =cv.glm(ks_sam_te,glm_ks_sam)
```
```{r}
sprintf("The misclassification rate of the training set is: raw: %f, adjusted: %f.", cv.err$delta[1], cv.err$delta[2])
sprintf("The misclassification rate of the test set is: raw: %f, adjusted: %f.", cv.err.test$delta[1], cv.err.test$delta[2])
```

The adjusted error rate of the training and test set resulting from the LOOCV is quite similar. But the raw cross-validation estimate of prediction error is much higher because there's not enough data in the test set so that the model trained every time is likely to be not fitted. And since every time it's trained on approximately identical data sets, the models are correlated with each other, adding on the effect of high variance.

#### Step 9: Explanations

Compare the misclassification rates from the simple method to the LOOCV method?

The misclassification rates from the simple method is 0.09 compared to  from the LOOCV method


How do data scientists really use cross-validation? How is the approach in this project differ from real-world cases? Give an example to make your point!

In this project, we applied cross-validation to the training set and test set respectively to compare the misclassification rate calculated. However, in real-world cases, cross-validation is used when there are not enough data points but we want to estimate the out-of-sample fit to select the best model so that we don't need to divide the training set and the test set.


## Extra Credit: Least Absolute Deviation Estimator

#### STEP 1

Figure out how to use rgenoud to run a regression that maximizes the least absolute deviation instead of the traditional **sum of the squared residuals**. Show that this works by running that regression on the `lalonde` data set with outcome being `re78` and independent variables being `age`, `education`, `hisp`, `re74`, `re75`, and `treat`. 

```{r}
# the least absolute deviation minimizes the sum of absolute errors
# meanerror <- function(beta0, beta1, x, y) {
#  abs(y - beta0 - beta1 * x)
#}

# result <- genoud(meanerror, nvars=1, max=TRUE, pop.size=3000)


```


#### STEP 2

How different is this coef on treat from the coef on treat that you get from the corresponding traditional least squares regression?





#### STEP 3

Now figure out how to do the same by using rgenoud to run the logistic regression (modeling treatment status as a function of `age`, `education`, `hisp`, `re74` and `re75`).

```{r}
# YOUR CODE HERE

```


## END OF Assignment!!!

## Final Steps

### Add Markdown Text to .Rmd

Before finalizing your project you'll want be sure there are **comments in your code chunks** and **text outside of your code chunks** to explain what you're doing in each code chunk. These explanations are incredibly helpful for someone who doesn't code or someone unfamiliar to your project.
You have two options for submission:

1. You can complete this .rmd file, knit it to pdf and submit both the .rmd file and the .pdf file on Forum as one .zip file.
2. You can submit your assignment as a separate pdf using your favorite text editor and submit the pdf file along with a lint to your github code. Note that links to Google Docs are not accepted.


### Knitting your R Markdown Document

Last but not least, you'll want to **Knit your .Rmd document into an HTML document**. If you get an error, take a look at what the error says and edit your .Rmd document. Then, try to Knit again! Troubleshooting these error messages will teach you a lot about coding in R. If you get any error that doesn't make sense to you, post it on Perusall.

### A Few Final Checks

If you are submitting an .rmd file, a complete project should have:

- Completed code chunks throughout the .Rmd document (your RMarkdown document should Knit without any error)
- Comments in your code chunks
- Answered all questions throughout this exercise.

If you are NOT submitting an .rmd file, a complete project should have:

- A pdf that includes all the answers and their questions.
- A link to Github (gist or repository) that contais all the code used to answer the questions. Each part of you code should say which question it's referring to.